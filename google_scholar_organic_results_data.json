[{"title":"Mobivqa: Efficient on-device visual question answering","title_link":"https:\/\/dl.acm.org\/doi\/abs\/10.1145\/3534619","publication_info":"Q Cao, P Khanna, ND Lane\u2026\u00a0- Proceedings of the ACM\u00a0\u2026, 2022 - dl.acm.org","snippet":"\u2026 much cross-modal processing is needed for different visual questions. In the first stage, we \ndecide if the input visual questions is too difficult to answer, in which case we fail early and do \u2026","cited_by_link":"https:\/\/scholar.google.com\/scholar?cites=6118359897258390539&as_sdt=2005&sciodt=0,5&hl=en","cited_by_count":14.0,"pdf_file":"https:\/\/dl.acm.org\/doi\/pdf\/10.1145\/3534619"},{"title":"POP-VQA-Privacy Preserving, On-Device, Personalized Visual Question Answering","title_link":"https:\/\/openaccess.thecvf.com\/content\/WACV2024\/html\/Sahu_POP-VQA_-_Privacy_Preserving_On-Device_Personalized_Visual_Question_Answering_WACV_2024_paper.html","publication_info":"PP Sahu, A Raut, JS Samant\u2026\u00a0- Proceedings of the\u00a0\u2026, 2024 - openaccess.thecvf.com","snippet":"\u2026 inferencing not only preserves user data privacy (as no information is moved out of device), \nbut also provides users with limited internet access (especially in low resource locations), an \u2026","cited_by_link":"https:\/\/scholar.google.com\/scholar?cites=7103689000804614270&as_sdt=2005&sciodt=0,5&hl=en","cited_by_count":3.0,"pdf_file":"https:\/\/openaccess.thecvf.com\/content\/WACV2024\/papers\/Sahu_POP-VQA_-_Privacy_Preserving_On-Device_Personalized_Visual_Question_Answering_WACV_2024_paper.pdf"},{"title":"Deqa: On-device question answering","title_link":"https:\/\/dl.acm.org\/doi\/abs\/10.1145\/3307334.3326071","publication_info":"Q Cao, N Weber, N Balasubramanian\u2026\u00a0- Proceedings of the 17th\u00a0\u2026, 2019 - dl.acm.org","snippet":"\u2026 We design DeQA (pronounced de-ka), which stands for on-device QA, that runs completely \nlocally \u2026 As users access new content, the search engine needs to store the data and index it. \u2026","cited_by_link":"https:\/\/scholar.google.com\/scholar?cites=2647407184993372916&as_sdt=2005&sciodt=0,5&hl=en","cited_by_count":23.0,"pdf_file":"https:\/\/dl.acm.org\/doi\/pdf\/10.1145\/3307334.3326071"},{"title":"TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices","title_link":"https:\/\/arxiv.org\/abs\/2404.03574","publication_info":"HA Rashid, A Sarkar, A Gangopadhyay\u2026\u00a0- arXiv preprint arXiv\u00a0\u2026, 2024 - arxiv.org","snippet":"\u2026 MobiVQA [3] proposed on-device VQA, focusing on early exit and selective processing \nand \u2026 stored in L2 memory for direct accessibility by the HWCE, streamlining convolutional \u2026","cited_by_link":"https:\/\/scholar.google.com\/scholar?cites=4045978822658502185&as_sdt=2005&sciodt=0,5&hl=en","cited_by_count":1.0,"pdf_file":"https:\/\/arxiv.org\/pdf\/2404.03574"},{"title":"[PDF][PDF] Robust Visual Question-Answering using Generative Vision Language Models","title_link":"https:\/\/cdn.iiit.ac.in\/cdn\/web2py.iiit.ac.in\/research_centres\/publications\/download\/mastersthesis.pdf.b4a20aa786ea92f1.526168756c4d656874615f5468657369732e706466.pdf","publication_info":"R Mehta - 2024 - cdn.iiit.ac.in","snippet":"\u2026 IIIT Hyderabad provided me with access to a leading research community in India and to \u2026 \nWe release a visual question answering (VQA) system for electrical circuit images that could be \u2026","cited_by_link":null,"cited_by_count":null,"pdf_file":"https:\/\/cdn.iiit.ac.in\/cdn\/web2py.iiit.ac.in\/research_centres\/publications\/download\/mastersthesis.pdf.b4a20aa786ea92f1.526168756c4d656874615f5468657369732e706466.pdf"},{"title":"SeeSay: An Assistive Device for the Visually Impaired Using Retrieval Augmented Generation","title_link":"https:\/\/arxiv.org\/abs\/2410.03771","publication_info":"M Yu\u00a0- arXiv preprint arXiv:2410.03771, 2024 - arxiv.org","snippet":"\u2026 application that can use visual querying models to provide accessibility guidance for blind or \n\u2026 cloud-based LLM services and boost user privacy by enabling on-device LLM functionality. \u2026","cited_by_link":null,"cited_by_count":null,"pdf_file":"https:\/\/arxiv.org\/pdf\/2410.03771"},{"title":"When can accessibility help? An exploration of accessibility feature recommendation on mobile devices","title_link":"https:\/\/dl.acm.org\/doi\/abs\/10.1145\/3430263.3452434","publication_info":"J Wu, G Reyes, SC White, X Zhang\u2026\u00a0- Proceedings of the 18th\u00a0\u2026, 2021 - dl.acm.org","snippet":"\u2026 As we will present, many of the accessibility features available on today\u2019s smartphones can \nbe activated using such mechanisms derived from behaviors detected based on device use. \u2026","cited_by_link":"https:\/\/scholar.google.com\/scholar?cites=12216786632015464583&as_sdt=2005&sciodt=0,5&hl=en","cited_by_count":18.0,"pdf_file":"https:\/\/dl.acm.org\/doi\/pdf\/10.1145\/3430263.3452434"},{"title":"Backpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration","title_link":"https:\/\/dl.acm.org\/doi\/abs\/10.1145\/3706422","publication_info":"W Ji, L Li, Z Lv, W Zhang, M Li, Z Wan, W Lei\u2026\u00a0- ACM Transactions on\u00a0\u2026, 2025 - dl.acm.org","snippet":"\u2026 , particularly in video question answering and retrieval tasks, \u2026 tailored for multimodal on-device \nmodel adaptation via cloud-\u2026 and copyright concerns, where access to the source domain \u2026","cited_by_link":"https:\/\/scholar.google.com\/scholar?cites=10542831946951094357&as_sdt=2005&sciodt=0,5&hl=en","cited_by_count":3.0,"pdf_file":"https:\/\/dl.acm.org\/doi\/pdf\/10.1145\/3706422"},{"title":"Vision-Language Models for Edge Networks: A Comprehensive Survey","title_link":"https:\/\/arxiv.org\/abs\/2502.07855","publication_info":"A Sharshar, LU Khan, W Ullah, M Guizani\u00a0- arXiv preprint arXiv\u00a0\u2026, 2025 - arxiv.org","snippet":"\u2026 captioning, visual question answering, and visual content \u2026 to make sophisticated VLMs \naccessible and practical for edge \u2026 online inference with minimal on-device memory is achieved \u2026","cited_by_link":"https:\/\/scholar.google.com\/scholar?cites=5927344814644833156&as_sdt=2005&sciodt=0,5&hl=en","cited_by_count":4.0,"pdf_file":"https:\/\/arxiv.org\/pdf\/2502.07855"},{"title":"On-device language models: A comprehensive review","title_link":"https:\/\/arxiv.org\/abs\/2409.00088","publication_info":"J Xu, Z Li, W Chen, Q Wang, X Gao, Q Cai\u2026\u00a0- arXiv preprint arXiv\u00a0\u2026, 2024 - arxiv.org","snippet":"\u2026 to on-device language models, providing insights into how this shift could redefine the landscape \nof applications and AI accessibility. \u2026 on-device, architectural foundations, and on-device \u2026","cited_by_link":"https:\/\/scholar.google.com\/scholar?cites=15282722275301821304&as_sdt=2005&sciodt=0,5&hl=en","cited_by_count":39.0,"pdf_file":"https:\/\/arxiv.org\/pdf\/2409.00088"}]