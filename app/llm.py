import asyncio
import os
import re
import aiohttp
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI

load_dotenv()


llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=1,
    google_api_key=os.getenv("GOOGLE_API_KEY_gemini"),
)

llm2 = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=1,
    google_api_key=os.getenv("GOOGLE_API_KEY_gemini2"),
)

ollama_models = [
    "gemma3:27b-90k",
    "gemma3:12b-it-fp16-45k",
    "gemma3:12b-it-fp16-90k",
]

# Your LLM logic goes here
async def invoke_llm(prompt, model):
    """
    Asynchronously invokes a language model (LLM) with the given prompt and model name, handling both Ollama and Google Gemini models.
    Args:
        prompt (str): The input prompt to send to the LLM.
        model (str): The name of the model to use for inference.
    Returns:
        str: The cleaned response generated by the LLM, or an error message if all attempts fail.
    Notes:
        - For Ollama models, the function makes up to 4 HTTP POST attempts to the LLM server, with retry logic on failure.
        - For Google Gemini models, the function invokes the model directly.
        - The output is cleaned to remove special tokens, excessive whitespace, and formatting artifacts.
    """

    raw_text = ""
    max_retries = 4

    if model in ollama_models:
        # using ollama gemma
        print(f"Using model: {model}")
        payload = {"prompt": prompt}
        url = f"{os.getenv('LLM_URL')}/query?model={model}"

        for attempt in range(1, max_retries + 1):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(url, json=payload) as response:
                        if response.status != 200:
                            text = await response.text()
                            print(f"Attempt {attempt}: Error {response.status} - {text}")
                            await asyncio.sleep(8)
                            continue

                        data = await response.json()
                        raw_text = data.get("content", "")
                        break
            except (aiohttp.ClientError, asyncio.CancelledError) as e:
                print(f"Attempt {attempt}: HTTP error - {str(e)}")
                await asyncio.sleep(10)
        else:
            return "LLM failed after 4 attempts."

    else:
        # using google gemini
        print(f"Using model: {model}")
        response = llm.invoke(prompt)
        raw_text = response.content

    # Clean output
    cleaned = re.sub(r"<think>.*?</think>", "", raw_text, flags=re.DOTALL | re.IGNORECASE)
    cleaned = re.sub(r"\*+", "", cleaned)
    cleaned = re.sub(r"#", "", cleaned)
    cleaned = re.sub(r"\s+", " ", cleaned)
    cleaned = cleaned.replace('")', '"')
    if "`" in cleaned:
        cleaned = cleaned[cleaned.find("`") :]
    cleaned = cleaned.strip()
    cleaned = cleaned.replace("```json", "").replace("```", "").strip()

    return cleaned
