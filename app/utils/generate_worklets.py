from app.utils.llm_response_parser import extract_json_from_llm_response
from app.llm import invoke_llm
from app.utils.prompt_templates import worklet_gen_prompt,worklet_gen_prompt_with_web_searches
from app.socket import sio
from app.utils.search_functions.search import search 
import asyncio

async def generate_worklets(worklet_data, linksData, model, sid):
    """
    Asynchronously generates worklets based on the provided data and model.
    This function interacts with a language model (LLM) to generate worklets 
    using the given `worklet_data` and `linksData`. If the LLM requests additional 
    web search data, it performs a web search and regenerates the worklets with 
    the search results.
    Args:
        worklet_data (dict): The primary data used for generating worklets.
        linksData (dict): Additional link-related data to be included in the prompt.
        model (str): The name of the LLM model to be used for generating worklets.
        sid (str): The session ID for emitting progress and error messages via Socket.IO.
    Returns:
        list: A list of extracted worklets generated by the LLM.
    Raises:
        Emits error messages via Socket.IO in case of:
            - LLM not responding.
            - Incorrect output format returned by the LLM.
    """
    count = 6
    count_string = "six"
    if model == "gemini-flash-2.0":
        count = 5
        count_string = "five"
    
    prompt_template = worklet_gen_prompt()
    prompt = prompt_template.format(worklet_data=worklet_data,linksData=linksData, count=count, count_string=count_string) # populte the prompt with worklet data
    
    try:
        generated_worklets = invoke_llm(prompt, model)
    except Exception as e:
        await sio.emit("error", {"message": "ERROR: LLM is not responding. Please try again."}, to=sid)

    extracted_worklets = []
    try:
        extracted_worklets = extract_json_from_llm_response(generated_worklets)# remove back ticks
    except Exception as e:
        await sio.emit("error", {"message": "ERROR: Wrong output returned by llm. Please try again."}, to=sid)

    if extracted_worklets["websearch"]:
        await sio.emit("progress", {"message": "LLM requested for web search..."}, to=sid)
        await asyncio.sleep(0.7)
        await sio.emit("progress", {"message": "Searching the web..."}, to=sid)

        s = search(extracted_worklets["search"],6, sid)
        await sio.emit("progress", {"message": "Generating worklets with web search results..."}, to=sid)
        prompt = worklet_gen_prompt_with_web_searches(json= s,count_string=count_string,count=count,context=extracted_worklets["current_context"])

        try:
            generated_worklets = invoke_llm(prompt, model)
        except Exception as e:
            await sio.emit("error", {"message": "ERROR: LLM is not responding. Please try again."}, to=sid)

        extracted_worklets = []
        
        try:
            extracted_worklets = extract_json_from_llm_response(generated_worklets)# remove back ticks
        except Exception as e:
                await sio.emit("error", {"message": "ERROR: Wrong output returned by llm. Please try again."}, to=sid)


    return extracted_worklets
