[
    {
        "Title": "Real-Time Video Call Anti-Aliasing using Generative AI",
        "Problem Statement": "Develop an on-device Generative AI model to reduce aliasing artifacts in real-time video calls, enhancing visual quality on mobile devices with limited bandwidth and processing power. Focus on balancing quality and latency.",
        "Description": "Video calls often suffer from aliasing, especially with low bandwidth. Existing anti-aliasing techniques are computationally expensive. This project explores leveraging generative models (e.g., GANs, Diffusion models) to reconstruct high-quality video frames, mitigating aliasing in real-time.",
        "Challenge / Use Case": "Improving user experience during video conferencing, particularly in challenging network conditions; enabling clearer communication and reducing eye strain.",
        "Deliverables": "Android app prototype with real-time anti-aliasing; Fine-tuned generative model; Performance evaluation report; Dataset of aliased/de-aliased video pairs.",
        "KPIs": [
            "PSNR \u2265 30dB",
            "Inference Latency \u2264 150ms on a mid-range smartphone",
            "Model Size \u2264 50MB",
            "Subjective visual quality score \u2265 4.0 (on a scale of 1-5)"
        ],
        "Prerequisites": [
            "Deep Learning basics (PyTorch/TensorFlow)",
            "Generative Adversarial Networks (GANs)",
            "Video processing fundamentals",
            "Android development experience",
            "Image Quality Metrics (PSNR, SSIM)",
            "TFLite model optimization"
        ],
        "Infrastructure Requirements": "Standard laptop with a GPU (NVIDIA GTX 1660 or better recommended); Android smartphone for testing; access to a large video dataset.",
        "Tentative Tech Stack": "Python, TensorFlow/PyTorch, OpenCV, Android Studio, TFLite, FFmpeg",
        "Milestones (6 months)": {
            "M2": "Dataset creation and preprocessing; Baseline GAN implementation.",
            "M4": "Model training and optimization; initial Android integration.",
            "M6": "Final app development, performance evaluation, and report writing."
        },
        "Reference Work": [
            {
                "Title": "Delving deeper into anti-aliasing in convnets",
                "Link": "https://link.springer.com/article/10.1007/s11263-022-01672-y",
                "Description": "\u2026 260 videos in the training set to validate video consistency. \u2026 in video, we propose a new \nmean Average Video Instance \u2026 results of applying our approach to generative models. In Fig. 11, \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Suppressing High-Frequency Artifacts for Generative Model Watermarking by Anti-Aliasing",
                "Link": "https://dl.acm.org/doi/abs/10.1145/3658664.3659634",
                "Description": "\u2026 for generative model watermarking. \u2022 To improve the imperceptibility of the generative \u2026 \nFor this purpose, we propose to exploit the anti-aliasing strategy for developing a watermark \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Analysis and solution to aliasing artifacts in neural waveform generation models",
                "Link": "https://www.sciencedirect.com/science/article/pii/S0003682X22005576",
                "Description": "In recent years, with the application of deep learning in speech synthesis, waveform \ngeneration models based on generative adversarial networks have achieved high quality \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Alias-free generative adversarial networks",
                "Link": "https://proceedings.neurips.cc/paper/2021/hash/076ccd93ad68be51f23707988e934906-Abstract.html",
                "Description": "\u2026 process of typical generative adversarial networks depends \u2026 for generative models better \nsuited for video and animation. \u2026 videos highlight the practical relevance of different dB values. \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Advancing Diffusion Models: Alias-Free Resampling and Enhanced Rotational Equivariance",
                "Link": "https://arxiv.org/abs/2411.09174",
                "Description": "\u2026 to incorporate them into video-generating diffusion models, \u2026 Thus, we have the following \nparameters for the anti-aliasing \u2026 to be a bottleneck for such generative models, our work offers \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Revisiting light field rendering with deep anti-aliasing neural network",
                "Link": "https://ieeexplore.ieee.org/abstract/document/9406381/",
                "Description": "\u2026 Please also refer to the submitted supplementary video for more qualitative results, \u2026 \nThe other difference is that we introduce a discrimination network to construct a generative \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Applications and Limitations of Machine Learning in Computer Graphics",
                "Link": "https://www.atlantis-press.com/proceedings/dai-23/125998062",
                "Description": "\u2026 in many fields especially in video games, films, virtual reality (\u2026 learning applications within \nanti-aliasing, ambient occlusion, \u2026 AOGAN: A generative adversarial network for screen space \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space",
                "Link": "https://arxiv.org/abs/2503.09419",
                "Description": "\u2026 The diffusion model is a family of generative models that \u2026 We hypothesize that this is because \nof the imperfect anti-aliasing \u2026 Then, we regenerate a video from the inverted latents with a \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Subpixel Deblurring of Anti\u2010Aliased Raster Clip\u2010Art",
                "Link": "https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14744",
                "Description": "\u2026 Image generative models including ours, do not explicitly minimize the color palette size \u2026 to \nthe problem we address fail to remove anti-aliasing blur and often generate outputs which are \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Evaluation of real-time aliasing reduction methods in neural networks for nonlinear audio effects modelling",
                "Link": "https://www.aes.org/e-lib/browse.cfm?elib=22384",
                "Description": "\u2026 of anti-aliasing methods for use in real-time. Notably, one method of anti-aliasing capable of \n\u2026 [ 12 ] present a number of methods of ren - dering the generator of a Generative Adversarial \u2026",
                "Tag": "scholar"
            }
        ]
    },
    {
        "Title": "On-Device Voice Activity Detection with Edge AI",
        "Problem Statement": "Design and implement an efficient, on-device Voice Activity Detection (VAD) system for smartphones, utilizing lightweight machine learning models and edge computing principles to minimize latency and power consumption.",
        "Description": "VAD is crucial for voice assistants and communication apps. Cloud-based VAD introduces latency and privacy concerns. This project focuses on developing a VAD model that runs entirely on the device, utilizing neural networks and optimized algorithms for resource-constrained environments.",
        "Challenge / Use Case": "Enhance responsiveness and privacy of voice-controlled applications; enable offline voice commands.",
        "Deliverables": "Android app demonstrating on-device VAD; Trained and optimized ML model; Performance benchmark report; Documentation on model training and deployment.",
        "KPIs": [
            "Accuracy \u2265 90% in detecting voice activity",
            "False Positive Rate \u2264 5%",
            "Inference Latency \u2264 80ms on a mid-range smartphone",
            "Model Size \u2264 20MB"
        ],
        "Prerequisites": [
            "Digital Signal Processing (DSP) basics",
            "Machine Learning fundamentals",
            "Audio feature extraction (MFCC, spectrogram)",
            "Android development experience",
            "Model quantization and optimization techniques",
            "Edge AI frameworks (TFLite Micro)"
        ],
        "Infrastructure Requirements": "Standard laptop; Android smartphone for testing; Access to a voice dataset (e.g., LibriSpeech, Common Voice).",
        "Tentative Tech Stack": "Python, TensorFlow/PyTorch, Android Studio, TFLite Micro, Librosa.",
        "Milestones (6 months)": {
            "M2": "Data collection and feature engineering; Baseline model training.",
            "M4": "Model optimization and Android integration.",
            "M6": "Performance evaluation and report writing."
        },
        "Reference Work": [
            {
                "Title": "Personal VAD 2.0: Optimizing personal voice activity detection for on-device speech recognition",
                "Link": "https://arxiv.org/abs/2204.03793",
                "Description": "\u2026 Personalization of on-device speech recognition (ASR) has seen explosive \u2026 voice activity \ndetector that detects the voice activity of a target speaker, as part of a streaming on-device ASR \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Personal VAD: Speaker-conditioned voice activity detection",
                "Link": "https://arxiv.org/abs/1908.04284",
                "Description": "\u2026 \u201d, a system to detect the voice activity of a target speaker at the frame level. This system is \nuseful for gating the inputs to a streaming on-device speech recognition system, such that it \u2026",
                "Tag": "scholar"
            },
            {
                "Title": " Optimizing Voice Activity Detection for Noisy Conditions.",
                "Link": "https://www.isca-archive.org/interspeech_2019/lin19c_interspeech.pdf",
                "Description": "\u2026 attention on how to improve Voice Activity Detection (VAD) in \u2026 ) and 88.6% inference accuracy \non device.We also compress \u2026 we present experimental and on-device test findings and ana\u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Voice Activity Detection (VAD) in Noisy Environments",
                "Link": "https://arxiv.org/abs/2312.05815",
                "Description": "\u2026 Personal VAD:Speaker-Conditioned Voice Activity Detection \u2026 voice activity detection at \nthe frame level. The system\u2019s novelty lies in its capacity to trigger on-device speech recognition \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Integrating Voice Activity Detection to Enhance Robustness of On-Device Speaker Verification",
                "Link": "https://link.springer.com/chapter/10.1007/978-981-96-0125-7_31",
                "Description": "\u2026 Therefore, we propose a voice activity detection (VAD) model to resilient the silence \nsegments in real-world scenarios. Experiments on Vietnamese voice datasets show that the \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness",
                "Link": "https://arxiv.org/abs/2406.09443",
                "Description": "\u2026 Different fusion-based models suit specific use cases: CLF or EF for devices with minimal \nlatency and limited on-device requirements, and DCLF for devices prioritizing accuracy. \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Radiovad: mmwave-based noise and interference-resilient voice activity detection",
                "Link": "https://ieeexplore.ieee.org/abstract/document/10509864/",
                "Description": "\u2026 an mmWave-based voice-activity detection system can mitigate \u2026 -based system can extract \nthe voice activity of the speaker and \u2026 voice activity detection for on-device speech recognition,\u201d \u2026",
                "Tag": "scholar"
            },
            {
                "Title": " Online speech recognition using multichannel parallel acoustic score computation and deep neural network (DNN)-based voice-activity detector",
                "Link": "https://www.mdpi.com/2076-3417/10/12/4091",
                "Description": "\u2026 Therefore, the trade-off between accuracy and real-time performance can be relaxed \nwhen compared to on-device deployment. However, issues on response latency or multiple \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "On-device system for device directed speech detection for improving human computer interaction",
                "Link": "https://ieeexplore.ieee.org/abstract/document/9543684/",
                "Description": "\u2026 We determined the voice activity of the utterance using WebRTC-VAD.We applied VAD (Voice \nActivity Detection) with the aggressiveness of 2 on the audio to create a vector of speech \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Hybrid voice activity detection system based on LSTM and auditory speech features",
                "Link": "https://www.sciencedirect.com/science/article/pii/S174680942200862X",
                "Description": "\u2026 Voice Activity Detection (VAD), sometimes called as Speech Activity Detection, is the process \nof extracting speech regions in audio recordings including many type of sounds. Because \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "cobra",
                "Description": "On-device voice activity detection (VAD) powered by deep learning",
                "Link": "https://github.com/Picovoice/cobra",
                "Tag": "github"
            }
        ]
    },
    {
        "Title": "Smart IoT Camera System with On-Device Object Detection",
        "Problem Statement": "Develop a smart IoT camera system capable of performing real-time object detection on the edge, reducing bandwidth requirements and improving privacy by processing data locally.",
        "Description": "Traditional IoT camera systems rely on cloud processing, leading to latency and privacy concerns. This project aims to create a self-contained system that can identify objects (e.g., people, vehicles) directly on the camera using lightweight deep learning models.",
        "Challenge / Use Case": "Security surveillance; Smart home automation; Real-time traffic monitoring.",
        "Deliverables": "IoT camera prototype with on-device object detection; Trained and optimized deep learning model; Performance evaluation report.",
        "KPIs": [
            "mAP \u2265 60% on a standard object detection dataset (e.g., COCO)",
            "Inference Latency \u2264 200ms on an embedded platform (e.g., Raspberry Pi)",
            "Power consumption \u2264 5W",
            "Frame Rate \u2265 15 FPS"
        ],
        "Prerequisites": [
            "Embedded systems programming",
            "Computer vision fundamentals",
            "Object detection algorithms (e.g., YOLO, SSD)",
            "Deep learning frameworks (TensorFlow/PyTorch)",
            "IoT communication protocols (e.g., MQTT)",
            "Model quantization and deployment on edge devices"
        ],
        "Infrastructure Requirements": "Raspberry Pi (or similar embedded platform); Camera module; Access to an object detection dataset.",
        "Tentative Tech Stack": "Python, TensorFlow/PyTorch, OpenCV, Raspberry Pi OS, MQTT.",
        "Milestones (6 months)": {
            "M2": "Data preprocessing and model training.",
            "M4": "Model optimization and deployment on Raspberry Pi.",
            "M6": "System integration and performance evaluation."
        },
        "Reference Work": [
            {
                "Title": "IoT-driven automated object detection algorithm for urban surveillance systems in smart cities",
                "Link": "https://ieeexplore.ieee.org/abstract/document/7931539/",
                "Description": "\u2026 They are key techniques in most of the traffic related IoT \u2026 unified method of automated object \ndetection for urban surveillance \u2026 Our proposed method can not only help to detect object \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Edgelens: Deep learning based object detection in integrated iot, fog and cloud computing environments",
                "Link": "https://ieeexplore.ieee.org/abstract/document/9036216/",
                "Description": "\u2026 and services based on Internet of Things (IoT) paradigm, require to process large amount \nof data in very less time. Among them surveillance and object detection have gained prime \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Internet of things with object detection: Challenges, applications, and solutions",
                "Link": "https://www.igi-global.com/chapter/internet-of-things-with-object-detection/224265",
                "Description": "\u2026 object can be predicted by comparing the current object \u2026 of object detection technique with \nInternet of Things (IoT) concept is \u2026 processing is to detect the most informative object from the \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "IoT-based 3D convolution for video salient object detection",
                "Link": "https://link.springer.com/article/10.1007/s00521-018-03971-3",
                "Description": "\u2026 The video salient object detection (SOD) is the first step for the devices in the Internet of \nThings (IoT) to understand the environment around them. The video SOD needs the objects\u2019 \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Object Detection using Learning Algorithm and IoT",
                "Link": "https://ieeexplore.ieee.org/abstract/document/10369005/",
                "Description": "\u2026 Detection device for Road Safety that makes use of IoT (Internet of Things) and the cutting-edge \nobject detection \u2026 of the Raspberry Pi 3 and the IoT (Internet of Things) to gather real-time \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "A smart IoT enabled end-to-end 3D object detection system for autonomous vehicles",
                "Link": "https://ieeexplore.ieee.org/abstract/document/9920700/",
                "Description": "\u2026 , we utilized YOLOv3 [5], the most active state-of-the-art object detectors for different 2D visual \napplications. We firstly utilized the YOLOv3 object detection model for 2D object detection. \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "SaliencyGAN: Deep learning semisupervised salient object detection in the fog of IoT",
                "Link": "https://ieeexplore.ieee.org/abstract/document/8859383/",
                "Description": "\u2026 Internet of Things (IoT), visual analysis and predictions are often performed by deep \nlearning models. Salient object detection (\u2026 To the best of our knowledge, this is the first IoT-oriented \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Object detection mechanism based on deep learning algorithm using embedded IoT devices for smart home appliances control in CoT",
                "Link": "https://link.springer.com/article/10.1007/s12652-019-01272-8",
                "Description": "\u2026 of object detection \u2026 shot detector (SSD) in IoT based embedded devices for smart home \nappliances control. We have developed a smart home automation system using object detection \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Ec\u00b2detect: Real-time online video object detection in edge-cloud collaborative iot",
                "Link": "https://ieeexplore.ieee.org/abstract/document/9771217/",
                "Description": "\u2026 online video object detection method. Specifically, we propose a trackingassisted object \ndetection \u2026 selection, where the accurate but heavy object detection is conducted by the Cloud on \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "IoT-Embedded Traffic Cones with CNN-based Object Detection to Roadwork Safety",
                "Link": "https://ieeexplore.ieee.org/abstract/document/10467840/",
                "Description": "\u2026 of roadwork by combining Internet of Things (IoT) devices with object recognition through \u2026 \ntraffic cones with integrated IoT devices. Traffic cones with IoT sensors track things like their \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Smart-Security-Camera",
                "Description": "IoT security camera running open-cv for object detection \ud83d\udcf9",
                "Link": "https://github.com/HackerShackOfficial/Smart-Security-Camera",
                "Tag": "github"
            }
        ]
    },
    {
        "Title": "AI-Powered Noise Cancellation for Mobile Communication",
        "Problem Statement": "Develop an on-device AI-powered noise cancellation system for mobile phones, effectively removing background noise during calls and recordings without significant latency or distortion.",
        "Description": "Traditional noise cancellation techniques often struggle with complex and dynamic noise environments. This project explores using deep learning models to learn and suppress noise more effectively, providing a clearer and more natural audio experience.",
        "Challenge / Use Case": "Improve call quality in noisy environments; Enhance audio recording clarity.",
        "Deliverables": "Android app with real-time noise cancellation; Trained deep learning model; Performance evaluation report.",
        "KPIs": [
            "Noise Reduction \u2265 15dB",
            "PESQ score \u2265 3.0",
            "Inference Latency \u2264 100ms on a mid-range smartphone",
            "Subjective audio quality score \u2265 4.0 (on a scale of 1-5)"
        ],
        "Prerequisites": [
            "Digital Signal Processing (DSP)",
            "Audio processing fundamentals",
            "Deep Learning concepts",
            "Android development experience",
            "Audio evaluation metrics (PESQ, STOI)"
        ],
        "Infrastructure Requirements": "Standard laptop; Android smartphone for testing; Access to a noisy audio dataset.",
        "Tentative Tech Stack": "Python, TensorFlow/PyTorch, Librosa, Android Studio.",
        "Milestones (6 months)": {
            "M2": "Dataset collection and preprocessing; Baseline noise cancellation implementation.",
            "M4": "Model training and optimization; Android integration.",
            "M6": "Performance evaluation and report writing."
        },
        "Reference Work": [
            {
                "Title": "Area and Energy Efficient Method Using AI for Noise Cancellation in Ear Phones",
                "Link": "https://ieeexplore.ieee.org/abstract/document/10331813/",
                "Description": "\u2026 Masse, \"DSP-based solution for ambient noise reduction in mobile phones,\" 1999 IEEE \nInternational Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Development and Proposal of Military Artificial Intelligence Battlefield Noise Cancellation Model for Secure Joint Operations",
                "Link": "https://link.springer.com/chapter/10.1007/978-3-031-54813-0_43",
                "Description": "\u2026 In line with this trend, the importance of operational security in wireless communication \u2026 \nin the academically unexplored field of improving the military command and communication \u2026",
                "Tag": "scholar"
            },
            {
                "Title": " AI-Driven Signal Processing for Mobile Communications",
                "Link": "https://www.researchgate.net/profile/Niranjana-Gurushankar/publication/388796793_AI-Driven_Signal_Processing_for_Mobile_Communications/links/67a65074207c0c20fa7e0e05/AI-Driven-Signal-Processing-for-Mobile-Communications.pdf",
                "Description": "\u2026 learning, and other AI paradigms are being applied to address \u2026 The integration of AI allows \nfor adaptive and intelligent signal \u2026 of AI-driven signal processing in mobile communications, \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "MUTE: Bringing IoT to noise cancellation",
                "Link": "https://dl.acm.org/doi/abs/10.1145/3230543.3230550",
                "Description": "\u2026 anti-noise signals near the human ears (eg, in Bose\u2019s noise cancellation headphones). \nThis paper brings IoT to active noise cancellation by combining wireless communication with \u2026",
                "Tag": "scholar"
            },
            {
                "Title": " Active noise cancellation in Microsoft teams using AI & NLP powered algorithms",
                "Link": "https://www.academia.edu/download/110358986/15123ijcsit03.pdf",
                "Description": "\u2026 since the use of mobile phones. The introduction of real-time background artificial intelligence \nnoise suppression to the Microsoft team has boosted communication because the new \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Towards an AI-driven universal anti-jamming solution with convolutional interference cancellation network",
                "Link": "https://arxiv.org/abs/2203.09717",
                "Description": "\u2026 We note that wireless communications aim at very low Bit Error Rate (BER typically below \n10\u22124) while ML systems are typically content with much lower performance. Therefore, the ML \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "AI Based Underwater Noise Cancellation to Reduce Ship-induced Marine Noise Pollution",
                "Link": "https://ieeexplore.ieee.org/abstract/document/10780205/",
                "Description": "\u2026 that mitigate vessel noise. This paper introduces an AI-driven device that leverages \nmachine learning algorithms to detect and locate sources of ship-generated noise. The device \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Networked sensing with AI-empowered interference management: Exploiting macro-diversity and array gain in perceptive mobile networks",
                "Link": "https://ieeexplore.ieee.org/abstract/document/10274487/",
                "Description": "\u2026 cancellation, but also enables networked sensing. However, there are also challenges \nfor integrating sensing into mobile \u2026 the two systems at both the device and network levels. \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Artificial intelligence in mobile communication: A Survey",
                "Link": "https://iopscience.iop.org/article/10.1088/1757-899X/1212/1/012046/meta",
                "Description": "\u2026 of mobile communication. This article describes brief AI approaches in mobile communication, \nseveral classics AI techniques, and the current AI approaches in wireless communication. \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "AI-enabled massive devices multiple access for smart city",
                "Link": "https://ieeexplore.ieee.org/abstract/document/8656575/",
                "Description": "\u2026 In the field of wireless communication with IoT, AI also has \u2026 -time AI-based scheduler of a \ncommunication network to allocate \u2026 In the following, we omit the insertion and removal of cyclic-\u2026",
                "Tag": "scholar"
            },
            {
                "Title": "ClearComm-NN",
                "Description": "This DNN project focuses on real-time noise cancellation and speech separation for mobile communications. It uses advanced AI to enhance clarity and distinguish voices in noisy settings, optimized for edge devices. This tech significantly improves mobile telecom, offering clear communication in noisy environments.",
                "Link": "https://github.com/Brandonio-c/ClearComm-NN",
                "Tag": "github"
            }
        ]
    },
    {
        "Title": "Smart Agriculture: Plant Disease Detection with Mobile Vision",
        "Problem Statement": "Develop a mobile application that leverages computer vision to detect plant diseases from images captured with a smartphone camera, providing farmers with timely and accurate diagnoses.",
        "Description": "Early detection of plant diseases is crucial for minimizing crop losses. This project aims to create a mobile app that can identify common plant diseases based on visual symptoms, empowering farmers to take appropriate action.",
        "Challenge / Use Case": "Improving crop yields; Reducing pesticide usage; Promoting sustainable agriculture.",
        "Deliverables": "Android app for plant disease detection; Trained deep learning model; Performance evaluation report.",
        "KPIs": [
            "Accuracy \u2265 85% on a standard plant disease dataset",
            "Inference Latency \u2264 300ms on a mid-range smartphone",
            "Number of supported diseases \u2265 5",
            "Usability score \u2265 4.0 (on a scale of 1-5)"
        ],
        "Prerequisites": [
            "Computer vision fundamentals",
            "Image classification algorithms (e.g., CNNs)",
            "Deep learning frameworks (TensorFlow/PyTorch)",
            "Android development experience"
        ],
        "Infrastructure Requirements": "Standard laptop; Android smartphone for testing; Access to a plant disease dataset.",
        "Tentative Tech Stack": "Python, TensorFlow/PyTorch, OpenCV, Android Studio.",
        "Milestones (6 months)": {
            "M2": "Dataset collection and preprocessing; Model training and evaluation.",
            "M4": "Android app development and integration.",
            "M6": "Testing and deployment."
        },
        "Reference Work": [
            {
                "Title": "Multi-resolution mobile vision system for plant leaf disease diagnosis",
                "Link": "https://link.springer.com/article/10.1007/s11760-015-0751-y",
                "Description": "\u2026 Designing and providing a fast-reliable automated mobile vision based solution for such \u2026 \nIn this paper, a mobile client\u2013server architecture for leaf disease detection and diagnosis using \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "CROPCARE: an intelligent real-time sustainable IoT system for crop disease detection using mobile vision",
                "Link": "https://ieeexplore.ieee.org/abstract/document/9526759/",
                "Description": "\u2026 crop disease detection and prevention system, called CROPCARE, is proposed. The \nsystem integrates mobile vision\u2026 system is to detect crop diseases through the CROPCARE\u2014mobile \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Energy efficient mobile vision system for plant leaf disease identification",
                "Link": "https://ieeexplore.ieee.org/abstract/document/6953083/",
                "Description": "\u2026 m-Agriculture projects are built on the opportunities provided by increasing use of mobile \u2026 \nmobile-based vision system for plant disease diagnosis via plant leaf imaging and analysis. \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "PMVT: a lightweight vision transformer for plant disease identification on mobile devices",
                "Link": "https://www.frontiersin.org/articles/10.3389/fpls.2023.1256773/full",
                "Description": "\u2026 based on the mobile vision transformer (MobileViT) for real-time detection of \u2026 plant \ndisease diagnosis app and successfully used the trained PMVT model to identify plant disease \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Plant disease detection using deep learning based Mobile application",
                "Link": "https://link.springer.com/article/10.1007/s11042-023-14541-8",
                "Description": "\u2026 MobileNet is a pre-trained model fine-tuned with deep neural networks explicitly designed \nfor the purpose of image classification and mobile vision applications. The main benefits of \u2026",
                "Tag": "scholar"
            },
            {
                "Title": " A mobile-based system for detecting plant leaf diseases using deep learning",
                "Link": "https://www.mdpi.com/2624-7402/3/3/32",
                "Description": "\u2026 paper presents an ML-powered mobile-based system to automate the plant leaf disease \u2026 , \nthe review of plant disease detection using machine learning [7] and computer vision [4] shows \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Automated plant leaf disease detection and classification using optimal MobileNet based convolutional neural networks",
                "Link": "https://www.sciencedirect.com/science/article/pii/S2214785321042115",
                "Description": "\u2026 the value of developing automatic plant disease detection models based on visible \u2026 \nautomated model for detecting and classifying plant leaf diseases using an optimal mobile network-\u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Automatic plant disease diagnosis using mobile capture devices, applied on a wheat use case",
                "Link": "https://www.sciencedirect.com/science/article/pii/S016816991631050X",
                "Description": "\u2026 Different research has addressed automated plant disease identification by computerized \nvisual diagnostic methods. Satellite or Airborne remote sensing have been proposed for \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "A mobile-based system for maize plant leaf disease detection and classification using deep learning",
                "Link": "https://www.frontiersin.org/articles/10.3389/fpls.2023.1079366/full",
                "Description": "\u2026 leaf accurately with a higher confidence score. YOLOv8n is the latest model used for the \ndetection of diseases \u2026 in a mobile application to provide a real-time disease detection facility for \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "MobilePlantViT: A Mobile-friendly Hybrid ViT for Generalized Plant Disease Image Classification",
                "Link": "https://arxiv.org/abs/2503.16628",
                "Description": "\u2026 performance in plant disease identification. However, deploying these models on mobile and \nedge \u2026 To address this, we propose MobilePlantViT, a novel hybrid Vision Transformer (ViT) \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "disease-detection",
                "Description": "Crop Disease Classification Training Model. This is a one part of the entire project. The complete project is a plant and pest detection mobile app using machine Learning algorithms and computer vision",
                "Link": "https://github.com/YounoussaBen/disease-detection",
                "Tag": "github"
            }
        ]
    },
    {
        "Title": "Real-time Gesture Recognition for Human-Computer Interaction",
        "Problem Statement": "Develop a system that can recognize hand gestures in real-time using a smartphone camera, enabling intuitive and touchless human-computer interaction.",
        "Description": "Traditional input methods can be cumbersome and limiting. This project aims to create a system that allows users to control devices and applications using natural hand gestures.",
        "Challenge / Use Case": "Accessibility; Gaming; Smart home control; Virtual reality.",
        "Deliverables": "Android app for gesture recognition; Trained deep learning model; Performance evaluation report.",
        "KPIs": [
            "Accuracy \u2265 80% on a standard gesture dataset",
            "Inference Latency \u2264 150ms on a mid-range smartphone",
            "Number of supported gestures \u2265 5",
            "Recognition range \u2265 0.5 meters"
        ],
        "Prerequisites": [
            "Computer vision fundamentals",
            "Hand gesture recognition algorithms",
            "Deep learning frameworks (TensorFlow/PyTorch)",
            "Android development experience"
        ],
        "Infrastructure Requirements": "Standard laptop; Android smartphone for testing; Access to a gesture dataset.",
        "Tentative Tech Stack": "Python, TensorFlow/PyTorch, OpenCV, Android Studio.",
        "Milestones (6 months)": {
            "M2": "Dataset collection and preprocessing; Model training and evaluation.",
            "M4": "Android app development and integration.",
            "M6": "Testing and deployment."
        },
        "Reference Work": [
            {
                "Title": "Gesture recognition: A survey",
                "Link": "https://ieeexplore.ieee.org/abstract/document/4154947/",
                "Description": "\u2026 The applications of gesture recognition are manifold, ranging from sign language through \u2026 \nprovide a survey on gesture recognition with particular emphasis on hand gestures and facial \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Hand gesture recognition with depth images: A review",
                "Link": "https://ieeexplore.ieee.org/abstract/document/6343787/",
                "Description": "\u2026 conceptual model of the major components of a hand gesture recognition system, shown in \nFigure 1. Depth-based hand gesture recognition begins with depth-image acquisition, which \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Vision-based gesture recognition: A review",
                "Link": "https://link.springer.com/chapter/10.1007/3-540-46616-9_10",
                "Description": "\u2026 A survey on recent vision-based gesture recognition approaches is given in this paper. We \n\u2026 temporal gesture recognition. Several application systems of gesture recognition are also \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Real-time gesture recognition system and application",
                "Link": "https://www.sciencedirect.com/science/article/pii/S0262885602001130",
                "Description": "\u2026 We have presented a modular approach to gesture recognition, where the gestures to be \nrecognized were decomposed into a smaller set of basic hand poses. A pose classifier was \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Hand gesture recognition: a literature review",
                "Link": "https://search.proquest.com/openview/17f752aaef608b544d1fcc2e80da3b53/1.pdf?pq-origsite=gscholar&cbl=646378",
                "Description": "\u2026 In this paper a survey of recent hand gesture recognition systems is presented. Key issues \nof hand gesture recognition system are presented with challenges of gesture system. Review \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Visual gesture recognition",
                "Link": "https://digital-library.theiet.org/doi/abs/10.1049/ip-vis%3A19941058",
                "Description": "\u2026 are then matched to stored gesture vector models using \u2026 recognition of seven gestures \nusing images sampled at 4 Hz on a SPARC-1 without any special hardware. The Seven gestures \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "A real-time hand gesture recognition method",
                "Link": "https://ieeexplore.ieee.org/abstract/document/4284820/",
                "Description": "\u2026 -time hand gesture recognition method. In our method, firstly, a specific gesture is required \u2026 \n-space feature detection is integrated into gesture recognition. Applying the proposed method \u2026",
                "Tag": "scholar"
            },
            {
                "Title": " Hand gesture recognition based on computer vision: a review of techniques",
                "Link": "https://www.mdpi.com/2313-433X/6/8/73",
                "Description": "\u2026 Where multi feature extracted and gesture recognition using \u2026 into gesture recognition in \norder to encounter the limitation of aspect ratio which facing most of the learning of hand gesture \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Sensors for gesture recognition systems",
                "Link": "https://ieeexplore.ieee.org/abstract/document/5976477/",
                "Description": "\u2026 capturing body gestures, the most popular being that of dynamic arm and hand gestures. \nA taxonomy of sensors is given in terms of their integration with a gesture recognition system (\u2026",
                "Tag": "scholar"
            },
            {
                "Title": "Gesture recognition",
                "Link": "https://link.springer.com/content/pdf/10.1007/978-3-030-63416-2_376.pdf",
                "Description": "Background Gait is increasingly used when a person cannot be identified by more conventional \nmeans: in the recent high-profile Hatton Gardens robbery in the UK,\u201cBasil\u201d covered his \u2026",
                "Tag": "scholar"
            },
            {
                "Title": "grt",
                "Description": "gesture recognition toolkit",
                "Link": "https://github.com/nickgillian/grt",
                "Tag": "github"
            }
        ]
    }
]